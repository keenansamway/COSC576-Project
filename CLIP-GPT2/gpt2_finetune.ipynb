{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetune GPT2 with AVA-Captions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used code from: https://rowlando13.medium.com/everything-gpt-2-5-fine-tuning-885aec508c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../CLIP-GPT2/models/gpt2-large\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Keenan Samway\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(train_path, test_path, tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=train_path,\n",
    "        block_size=128,\n",
    "    )\n",
    "    test_dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=test_path,\n",
    "        block_size=128,\n",
    "    )\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset, test_dataset, data_collator\n",
    "\n",
    "train_path = \"../CLIP-GPT2/data/large/train.txt\"\n",
    "test_path = \"../CLIP-GPT2/data/large/test.txt\"\n",
    "train_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=PreTrainedTokenizer(name_or_path='../CLIP-GPT2/models/gpt2-large', vocab_size=50257, model_max_len=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}), mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([39113,    13,  1201,   428,   318,  6190, 12857,  1312,   892,  1312,\n",
       "          561,   466,  1223,  3863, 21792,  9482,   284,   644,  3568,   284,\n",
       "          307,  6268, 23671,   319,   262,  1353,  8470,    13,  3073,   588,\n",
       "         7862,  8470,   284,   502,    13,  3621,  3124, 12019,    13,  7862,\n",
       "         8470,     0, 20105,    11,  1312,   561,  1842,   284,  3285,   511,\n",
       "         3496,     0,  1312,  1842, 19974,  8470,   612,   826,   510,   612,\n",
       "          351, 28494,  9154,  1276,   307,   617,  1611,   286,  5417,  7185,\n",
       "           13,   428,   329,   617,  1738, 17603,   502,   286,   257, 24276,\n",
       "        26842,    13, 44929,   286,   257,  6228, 11084, 26842,    30,  4168,\n",
       "          286,  1223,    13,  3297,   286, 23387,     0,   262, 44929,   286,\n",
       "          257, 26842,  5417,  2099,   262,  3793,   286,   257,  2636,  5156,\n",
       "        26842,    30,  3863,   257, 22647,   393,   257, 26842,   845,  1862,\n",
       "           13,  2508, 15756, 27179,  9124,    30, 29181,    11])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../CLIP-GPT2/models/gpt2-large-AVA\",\n",
    "    overwrite_output_dir=False,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=1,\n",
    "    eval_steps=5000,\n",
    "    save_steps=10000,\n",
    "    warmup_steps=500,\n",
    "    fp16=True,\n",
    "    fp16_opt_level=\"O1\",\n",
    "    prediction_loss_only=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-20000.\n",
      "c:\\Users\\Keenan Samway\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 219333\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 82251\n",
      "  Number of trainable parameters = 774030080\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 0\n",
      "  Continuing training from global step 20000\n",
      "  Will skip the first 0 epochs then the first 20000 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af93145e4494f749d4a4e14b4d6af33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ee04c28cbb47ee9d6194d66294eb65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/82251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2334, 'learning_rate': 3.7772015021222984e-05, 'epoch': 0.75}\n",
      "{'loss': 3.2254, 'learning_rate': 3.746620836442368e-05, 'epoch': 0.77}\n",
      "{'loss': 3.2262, 'learning_rate': 3.7160401707624375e-05, 'epoch': 0.78}\n",
      "{'loss': 3.228, 'learning_rate': 3.685459505082507e-05, 'epoch': 0.8}\n",
      "{'loss': 3.2226, 'learning_rate': 3.654878839402576e-05, 'epoch': 0.82}\n",
      "{'loss': 3.2257, 'learning_rate': 3.624359335054006e-05, 'epoch': 0.84}\n",
      "{'loss': 3.2301, 'learning_rate': 3.593778669374075e-05, 'epoch': 0.86}\n",
      "{'loss': 3.2246, 'learning_rate': 3.563198003694145e-05, 'epoch': 0.88}\n",
      "{'loss': 3.2214, 'learning_rate': 3.532617338014214e-05, 'epoch': 0.89}\n",
      "{'loss': 3.219, 'learning_rate': 3.5020366723342835e-05, 'epoch': 0.91}\n",
      "{'loss': 3.2168, 'learning_rate': 3.471456006654353e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2179, 'learning_rate': 3.4408753409744226e-05, 'epoch': 0.95}\n",
      "{'loss': 3.2115, 'learning_rate': 3.410294675294492e-05, 'epoch': 0.97}\n",
      "{'loss': 3.2056, 'learning_rate': 3.3797751709459214e-05, 'epoch': 0.98}\n",
      "{'loss': 3.1872, 'learning_rate': 3.349194505265991e-05, 'epoch': 1.0}\n",
      "{'loss': 3.049, 'learning_rate': 3.3186138395860605e-05, 'epoch': 1.02}\n",
      "{'loss': 3.0556, 'learning_rate': 3.28803317390613e-05, 'epoch': 1.04}\n",
      "{'loss': 3.0584, 'learning_rate': 3.2575136695575593e-05, 'epoch': 1.06}\n",
      "{'loss': 3.051, 'learning_rate': 3.226933003877628e-05, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-30000\n",
      "Configuration saved in ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-30000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0534, 'learning_rate': 3.196352338197698e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-30000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0554, 'learning_rate': 3.165771672517768e-05, 'epoch': 1.11}\n",
      "{'loss': 3.0569, 'learning_rate': 3.135191006837837e-05, 'epoch': 1.13}\n",
      "{'loss': 3.0541, 'learning_rate': 3.104671502489266e-05, 'epoch': 1.15}\n",
      "{'loss': 3.0515, 'learning_rate': 3.074090836809336e-05, 'epoch': 1.17}\n",
      "{'loss': 3.0587, 'learning_rate': 3.043510171129405e-05, 'epoch': 1.19}\n",
      "{'loss': 3.0526, 'learning_rate': 3.0129295054494748e-05, 'epoch': 1.2}\n",
      "{'loss': 3.0477, 'learning_rate': 2.9824100011009044e-05, 'epoch': 1.22}\n",
      "{'loss': 3.0596, 'learning_rate': 2.9518293354209737e-05, 'epoch': 1.24}\n",
      "{'loss': 3.0616, 'learning_rate': 2.921248669741043e-05, 'epoch': 1.26}\n",
      "{'loss': 3.0613, 'learning_rate': 2.890729165392472e-05, 'epoch': 1.28}\n",
      "{'loss': 3.0536, 'learning_rate': 2.8601484997125417e-05, 'epoch': 1.29}\n",
      "{'loss': 3.0499, 'learning_rate': 2.8295678340326116e-05, 'epoch': 1.31}\n",
      "{'loss': 3.0568, 'learning_rate': 2.7989871683526808e-05, 'epoch': 1.33}\n",
      "{'loss': 3.0621, 'learning_rate': 2.76840650267275e-05, 'epoch': 1.35}\n",
      "{'loss': 3.0647, 'learning_rate': 2.73782583699282e-05, 'epoch': 1.37}\n",
      "{'loss': 3.0504, 'learning_rate': 2.7072451713128895e-05, 'epoch': 1.39}\n",
      "{'loss': 3.0629, 'learning_rate': 2.6767256669643187e-05, 'epoch': 1.4}\n",
      "{'loss': 3.0477, 'learning_rate': 2.646145001284388e-05, 'epoch': 1.42}\n",
      "{'loss': 3.0567, 'learning_rate': 2.6155643356044575e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-40000\n",
      "Configuration saved in ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-40000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0491, 'learning_rate': 2.584983669924527e-05, 'epoch': 1.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-40000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0637, 'learning_rate': 2.5544641655759567e-05, 'epoch': 1.48}\n",
      "{'loss': 3.0558, 'learning_rate': 2.523883499896026e-05, 'epoch': 1.5}\n",
      "{'loss': 3.0693, 'learning_rate': 2.493302834216095e-05, 'epoch': 1.51}\n",
      "{'loss': 3.0586, 'learning_rate': 2.4627221685361647e-05, 'epoch': 1.53}\n",
      "{'loss': 3.047, 'learning_rate': 2.4321415028562342e-05, 'epoch': 1.55}\n",
      "{'loss': 3.0601, 'learning_rate': 2.4015608371763038e-05, 'epoch': 1.57}\n",
      "{'loss': 3.0444, 'learning_rate': 2.3709801714963733e-05, 'epoch': 1.59}\n",
      "{'loss': 3.0513, 'learning_rate': 2.3403995058164426e-05, 'epoch': 1.6}\n",
      "{'loss': 3.0502, 'learning_rate': 2.309880001467872e-05, 'epoch': 1.62}\n",
      "{'loss': 3.0457, 'learning_rate': 2.2792993357879417e-05, 'epoch': 1.64}\n",
      "{'loss': 3.0451, 'learning_rate': 2.248718670108011e-05, 'epoch': 1.66}\n",
      "{'loss': 3.0535, 'learning_rate': 2.2181991657594402e-05, 'epoch': 1.68}\n",
      "{'loss': 3.0484, 'learning_rate': 2.1876185000795098e-05, 'epoch': 1.7}\n",
      "{'loss': 3.0464, 'learning_rate': 2.1570378343995793e-05, 'epoch': 1.71}\n",
      "{'loss': 3.0548, 'learning_rate': 2.126457168719649e-05, 'epoch': 1.73}\n",
      "{'loss': 3.037, 'learning_rate': 2.095937664371078e-05, 'epoch': 1.75}\n",
      "{'loss': 3.0533, 'learning_rate': 2.0653569986911474e-05, 'epoch': 1.77}\n",
      "{'loss': 3.0497, 'learning_rate': 2.0347763330112173e-05, 'epoch': 1.79}\n",
      "{'loss': 3.0462, 'learning_rate': 2.0042568286626465e-05, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-50000\n",
      "Configuration saved in ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-50000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0386, 'learning_rate': 1.9736761629827157e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-50000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0491, 'learning_rate': 1.9430954973027853e-05, 'epoch': 1.84}\n",
      "{'loss': 3.0452, 'learning_rate': 1.912514831622855e-05, 'epoch': 1.86}\n",
      "{'loss': 3.044, 'learning_rate': 1.8819341659429244e-05, 'epoch': 1.88}\n",
      "{'loss': 3.0354, 'learning_rate': 1.851353500262994e-05, 'epoch': 1.9}\n",
      "{'loss': 3.0437, 'learning_rate': 1.8207728345830632e-05, 'epoch': 1.91}\n",
      "{'loss': 3.035, 'learning_rate': 1.7901921689031327e-05, 'epoch': 1.93}\n",
      "{'loss': 3.036, 'learning_rate': 1.7596115032232023e-05, 'epoch': 1.95}\n",
      "{'loss': 3.0398, 'learning_rate': 1.729030837543272e-05, 'epoch': 1.97}\n",
      "{'loss': 3.0367, 'learning_rate': 1.698450171863341e-05, 'epoch': 1.99}\n",
      "{'loss': 2.9716, 'learning_rate': 1.6678695061834106e-05, 'epoch': 2.01}\n",
      "{'loss': 2.8537, 'learning_rate': 1.63735000183484e-05, 'epoch': 2.02}\n",
      "{'loss': 2.8504, 'learning_rate': 1.6067693361549094e-05, 'epoch': 2.04}\n",
      "{'loss': 2.8578, 'learning_rate': 1.576188670474979e-05, 'epoch': 2.06}\n",
      "{'loss': 2.8566, 'learning_rate': 1.5456691661264083e-05, 'epoch': 2.08}\n",
      "{'loss': 2.8609, 'learning_rate': 1.5150885004464777e-05, 'epoch': 2.1}\n",
      "{'loss': 2.8527, 'learning_rate': 1.4845078347665472e-05, 'epoch': 2.12}\n",
      "{'loss': 2.8593, 'learning_rate': 1.453927169086617e-05, 'epoch': 2.13}\n",
      "{'loss': 2.8515, 'learning_rate': 1.4233465034066862e-05, 'epoch': 2.15}\n",
      "{'loss': 2.8575, 'learning_rate': 1.3928269990581156e-05, 'epoch': 2.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-60000\n",
      "Configuration saved in ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-60000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8618, 'learning_rate': 1.3622463333781852e-05, 'epoch': 2.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-60000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8531, 'learning_rate': 1.3316656676982545e-05, 'epoch': 2.21}\n",
      "{'loss': 2.851, 'learning_rate': 1.3010850020183241e-05, 'epoch': 2.22}\n",
      "{'loss': 2.8522, 'learning_rate': 1.2705043363383933e-05, 'epoch': 2.24}\n",
      "{'loss': 2.8534, 'learning_rate': 1.239984831989823e-05, 'epoch': 2.26}\n",
      "{'loss': 2.8613, 'learning_rate': 1.2094041663098923e-05, 'epoch': 2.28}\n",
      "{'loss': 2.8597, 'learning_rate': 1.1788235006299619e-05, 'epoch': 2.3}\n",
      "{'loss': 2.8528, 'learning_rate': 1.1482428349500312e-05, 'epoch': 2.32}\n",
      "{'loss': 2.8613, 'learning_rate': 1.1176621692701008e-05, 'epoch': 2.33}\n",
      "{'loss': 2.8557, 'learning_rate': 1.0870815035901702e-05, 'epoch': 2.35}\n",
      "{'loss': 2.8584, 'learning_rate': 1.0565008379102396e-05, 'epoch': 2.37}\n",
      "{'loss': 2.8589, 'learning_rate': 1.0259201722303091e-05, 'epoch': 2.39}\n",
      "{'loss': 2.8555, 'learning_rate': 9.954006678817384e-06, 'epoch': 2.41}\n",
      "{'loss': 2.8543, 'learning_rate': 9.64820002201808e-06, 'epoch': 2.43}\n",
      "{'loss': 2.8622, 'learning_rate': 9.342393365218773e-06, 'epoch': 2.44}\n",
      "{'loss': 2.8562, 'learning_rate': 9.03658670841947e-06, 'epoch': 2.46}\n",
      "{'loss': 2.8553, 'learning_rate': 8.731391664933762e-06, 'epoch': 2.48}\n",
      "{'loss': 2.848, 'learning_rate': 8.425585008134459e-06, 'epoch': 2.5}\n",
      "{'loss': 2.855, 'learning_rate': 8.12038996464875e-06, 'epoch': 2.52}\n",
      "{'loss': 2.8534, 'learning_rate': 7.814583307849446e-06, 'epoch': 2.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-70000\n",
      "Configuration saved in ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-70000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8531, 'learning_rate': 7.508776651050141e-06, 'epoch': 2.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-70000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8515, 'learning_rate': 7.202969994250836e-06, 'epoch': 2.57}\n",
      "{'loss': 2.8498, 'learning_rate': 6.8971633374515305e-06, 'epoch': 2.59}\n",
      "{'loss': 2.8517, 'learning_rate': 6.591356680652224e-06, 'epoch': 2.61}\n",
      "{'loss': 2.8583, 'learning_rate': 6.285550023852919e-06, 'epoch': 2.63}\n",
      "{'loss': 2.8487, 'learning_rate': 5.979743367053614e-06, 'epoch': 2.64}\n",
      "{'loss': 2.8498, 'learning_rate': 5.674548323567907e-06, 'epoch': 2.66}\n",
      "{'loss': 2.852, 'learning_rate': 5.368741666768602e-06, 'epoch': 2.68}\n",
      "{'loss': 2.8465, 'learning_rate': 5.062935009969298e-06, 'epoch': 2.7}\n",
      "{'loss': 2.8481, 'learning_rate': 4.757128353169992e-06, 'epoch': 2.72}\n",
      "{'loss': 2.8429, 'learning_rate': 4.451933309684286e-06, 'epoch': 2.74}\n",
      "{'loss': 2.8465, 'learning_rate': 4.1461266528849806e-06, 'epoch': 2.75}\n",
      "{'loss': 2.8453, 'learning_rate': 3.840319996085675e-06, 'epoch': 2.77}\n",
      "{'loss': 2.8446, 'learning_rate': 3.5345133392863696e-06, 'epoch': 2.79}\n",
      "{'loss': 2.8457, 'learning_rate': 3.2287066824870647e-06, 'epoch': 2.81}\n",
      "{'loss': 2.8396, 'learning_rate': 2.922900025687759e-06, 'epoch': 2.83}\n",
      "{'loss': 2.8548, 'learning_rate': 2.617093368888454e-06, 'epoch': 2.84}\n",
      "{'loss': 2.8454, 'learning_rate': 2.3118983254027477e-06, 'epoch': 2.86}\n",
      "{'loss': 2.838, 'learning_rate': 2.0060916686034424e-06, 'epoch': 2.88}\n",
      "{'loss': 2.8495, 'learning_rate': 1.7002850118041369e-06, 'epoch': 2.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-80000\n",
      "Configuration saved in ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-80000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8567, 'learning_rate': 1.3944783550048318e-06, 'epoch': 2.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-80000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8581, 'learning_rate': 1.0886716982055265e-06, 'epoch': 2.94}\n",
      "{'loss': 2.8443, 'learning_rate': 7.828650414062213e-07, 'epoch': 2.95}\n",
      "{'loss': 2.8479, 'learning_rate': 4.770583846069161e-07, 'epoch': 2.97}\n",
      "{'loss': 2.8435, 'learning_rate': 1.7125172780761094e-07, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 19054.3297, 'train_samples_per_second': 34.533, 'train_steps_per_second': 4.317, 'train_loss': 2.258287075406473, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=82251, training_loss=2.258287075406473, metrics={'train_runtime': 19054.3297, 'train_samples_per_second': 34.533, 'train_steps_per_second': 4.317, 'train_loss': 2.258287075406473, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../CLIP-GPT2/models/gpt2-large\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "\n",
    "m_loc = \"../CLIP-GPT2/models/gpt2_large-AVA/checkpoint-80000\"\n",
    "model = GPT2LMHeadModel.from_pretrained(m_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"I like this image, but ive seen a lot of them. i think this is the best of the lot. i really like the composition and the colors. the only thing i don't like is the border. i think it takes away from the simplicity of the shot. i\"},\n",
       " {'generated_text': 'I like this image, but ive seen so many of them now that its starting to get a little boring. this is a nice image. i really like the angle from which you shot this image. i also like the fact that you shot this image in black and white. nice'},\n",
       " {'generated_text': 'I like this image, but ive never been able to pull it off as well as you have here. i love the way the light is hitting the top of the glass, and the way it reflects on the table below. great job. i really like this one. the only'},\n",
       " {'generated_text': 'I like this image, but ive never been able to pull it off. great job. this is a great idea, but i think it would have been better if you could have gotten rid of the shadow in the bottom of the picture. good job though. i like the idea'},\n",
       " {'generated_text': \"I like this image, but ive seen so many of them that it's starting to get a little repetitive. i like the composition of this photo, but i'm not sure if i like the sepia effect. i think i would have liked to see it in color. the\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = \"I like this image, but \"\n",
    "tokens = tokenizer.encode(prefix)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "output = pipe(prefix, max_new_tokens=50, num_return_sequences=5, pad_token_id=50256, num_beams=5)\n",
    "\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a885d5bfa55cca12b0dd162160ddb2cadd7a03a3570ec17fb5426c28a2279b3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
