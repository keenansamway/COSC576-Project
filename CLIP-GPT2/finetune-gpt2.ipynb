{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c221c2aa8b94590b7a943bad268e6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Keenan Samway\\miniconda3\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Keenan Samway\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09dea60d4ef4e6fb142577d8717cb03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d72ee444104764addf22b2e4a6fb03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b53608811414892811d6f33cc0ff863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"../CLIP-GPT2/models/gpt2-large\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\")\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31188355 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "## DATA PREPERATION\n",
    "def file_to_list(text_loc, tokenizer, max_len):\n",
    "    text_blocks = []\n",
    "    \n",
    "    f = open(text_loc, encoding=\"utf-8\")\n",
    "    text = f.read()\n",
    "    \n",
    "    tokens = tokenizer.encode(text)\n",
    "    \n",
    "    while len(tokens) > 0:\n",
    "        holder = []\n",
    "        if len(tokens) > max_len:\n",
    "            holder = tokens[0:max_len]\n",
    "            del tokens[0:max_len]\n",
    "            if holder[-1] != 50256:\n",
    "                holder.append(50256)\n",
    "        \n",
    "        else:\n",
    "            holder = tokens\n",
    "            tokens = []\n",
    "            if holder[-1] != 50256:\n",
    "                holder.append(50256)\n",
    "                \n",
    "        text_blocks.append(tokenizer.decode(holder))\n",
    "    \n",
    "    return text_blocks\n",
    "\n",
    "def text_to_pieces(text_loc, tokenizer, max_len=1024):\n",
    "    text_blocks = file_to_list(text_loc, tokenizer, max_len)    \n",
    "    return text_blocks\n",
    "\n",
    "text_loc = \"../datasets/AVA/AVA-captions_clean_full_text.txt\"\n",
    "max_len = 1024\n",
    "\n",
    "data = text_to_pieces(text_loc, tokenizer, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loc = \"../CLIP-GPT2/data/large/train.txt\"\n",
    "test_loc = \"../CLIP-GPT2/data/large/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = \"\"\n",
    "train_text = train_text.join(train)\n",
    "\n",
    "test_text = \"\"\n",
    "test_text = test_text.join(test)\n",
    "\n",
    "with open(train_loc, \"x\", encoding=\"utf-8\") as f:\n",
    "    f.write(train_text)\n",
    "\n",
    "with open(test_loc, \"x\", encoding=\"utf-8\") as f:\n",
    "    f.write(test_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restart Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../CLIP-GPT2/models/gpt2-large\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Keenan Samway\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(train_path, test_path, tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=train_path,\n",
    "        block_size=128,\n",
    "    )\n",
    "    test_dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=test_path,\n",
    "        block_size=128,\n",
    "    )\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset, test_dataset, data_collator\n",
    "\n",
    "train_path = \"../CLIP-GPT2/data/large/train.txt\"\n",
    "test_path = \"../CLIP-GPT2/data/large/test.txt\"\n",
    "train_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../CLIP-GPT2/models/gpt2-large-AVA\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=1,\n",
    "    eval_steps=10000,\n",
    "    save_steps=10000,\n",
    "    warmup_steps=500,\n",
    "    fp16=True,\n",
    "    fp16_opt_level=\"O1\",\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Keenan Samway\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 219333\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 27417\n",
      "  Number of trainable parameters = 774030080\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621b64dfc9ae46fdbacca161a986a633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27417 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.636, 'learning_rate': 5e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4952, 'learning_rate': 4.907121893227329e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4521, 'learning_rate': 4.814243786454657e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4143, 'learning_rate': 4.7213656796819856e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3904, 'learning_rate': 4.628487572909314e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3747, 'learning_rate': 4.535609466136643e-05, 'epoch': 0.11}\n",
      "{'loss': 3.3704, 'learning_rate': 4.442731359363971e-05, 'epoch': 0.13}\n",
      "{'loss': 3.35, 'learning_rate': 4.3498532525913e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3434, 'learning_rate': 4.256975145818628e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3283, 'learning_rate': 4.164097039045956e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3286, 'learning_rate': 4.0712189322732844e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3247, 'learning_rate': 3.978340825500613e-05, 'epoch': 0.22}\n",
      "{'loss': 3.2998, 'learning_rate': 3.885462718727942e-05, 'epoch': 0.24}\n",
      "{'loss': 3.2964, 'learning_rate': 3.79258461195527e-05, 'epoch': 0.26}\n",
      "{'loss': 3.2925, 'learning_rate': 3.699892261396144e-05, 'epoch': 0.27}\n",
      "{'loss': 3.2859, 'learning_rate': 3.6070141546234727e-05, 'epoch': 0.29}\n",
      "{'loss': 3.2955, 'learning_rate': 3.514136047850801e-05, 'epoch': 0.31}\n",
      "{'loss': 3.2761, 'learning_rate': 3.4212579410781294e-05, 'epoch': 0.33}\n",
      "{'loss': 3.2783, 'learning_rate': 3.328379834305458e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-10000\n",
      "Configuration saved in ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-10000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2606, 'learning_rate': 3.235501727532786e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-10000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2604, 'learning_rate': 3.142623620760115e-05, 'epoch': 0.38}\n",
      "{'loss': 3.2666, 'learning_rate': 3.049745513987443e-05, 'epoch': 0.4}\n",
      "{'loss': 3.2629, 'learning_rate': 2.9570531634283166e-05, 'epoch': 0.42}\n",
      "{'loss': 3.242, 'learning_rate': 2.8641750566556453e-05, 'epoch': 0.44}\n",
      "{'loss': 3.2375, 'learning_rate': 2.771296949882974e-05, 'epoch': 0.46}\n",
      "{'loss': 3.251, 'learning_rate': 2.6784188431103023e-05, 'epoch': 0.47}\n",
      "{'loss': 3.241, 'learning_rate': 2.5857264925511758e-05, 'epoch': 0.49}\n",
      "{'loss': 3.2374, 'learning_rate': 2.4928483857785045e-05, 'epoch': 0.51}\n",
      "{'loss': 3.2233, 'learning_rate': 2.399970279005833e-05, 'epoch': 0.53}\n",
      "{'loss': 3.2201, 'learning_rate': 2.3070921722331612e-05, 'epoch': 0.55}\n",
      "{'loss': 3.2248, 'learning_rate': 2.214399821674035e-05, 'epoch': 0.57}\n",
      "{'loss': 3.2208, 'learning_rate': 2.1215217149013637e-05, 'epoch': 0.58}\n",
      "{'loss': 3.2228, 'learning_rate': 2.028643608128692e-05, 'epoch': 0.6}\n",
      "{'loss': 3.2131, 'learning_rate': 1.9357655013560204e-05, 'epoch': 0.62}\n",
      "{'loss': 3.2158, 'learning_rate': 1.8430731507968943e-05, 'epoch': 0.64}\n",
      "{'loss': 3.2092, 'learning_rate': 1.7501950440242226e-05, 'epoch': 0.66}\n",
      "{'loss': 3.2104, 'learning_rate': 1.6573169372515513e-05, 'epoch': 0.67}\n",
      "{'loss': 3.208, 'learning_rate': 1.5644388304788796e-05, 'epoch': 0.69}\n",
      "{'loss': 3.1979, 'learning_rate': 1.4717464799197533e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-20000\n",
      "Configuration saved in ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-20000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1949, 'learning_rate': 1.3790541293606272e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../CLIP-GPT2/models/gpt2-large-AVA\\checkpoint-20000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2123, 'learning_rate': 1.2861760225879557e-05, 'epoch': 0.75}\n",
      "{'loss': 3.1939, 'learning_rate': 1.193297915815284e-05, 'epoch': 0.77}\n",
      "{'loss': 3.1908, 'learning_rate': 1.1004198090426125e-05, 'epoch': 0.78}\n",
      "{'loss': 3.1903, 'learning_rate': 1.007541702269941e-05, 'epoch': 0.8}\n",
      "{'loss': 3.1844, 'learning_rate': 9.148493517108147e-06, 'epoch': 0.82}\n",
      "{'loss': 3.1863, 'learning_rate': 8.219712449381432e-06, 'epoch': 0.84}\n",
      "{'loss': 3.1891, 'learning_rate': 7.290931381654717e-06, 'epoch': 0.86}\n",
      "{'loss': 3.1839, 'learning_rate': 6.362150313928002e-06, 'epoch': 0.88}\n",
      "{'loss': 3.1807, 'learning_rate': 5.4333692462012855e-06, 'epoch': 0.89}\n",
      "{'loss': 3.1783, 'learning_rate': 4.506445740610023e-06, 'epoch': 0.91}\n",
      "{'loss': 3.1765, 'learning_rate': 3.5776646728833082e-06, 'epoch': 0.93}\n",
      "{'loss': 3.1778, 'learning_rate': 2.6488836051565926e-06, 'epoch': 0.95}\n",
      "{'loss': 3.172, 'learning_rate': 1.720102537429877e-06, 'epoch': 0.97}\n",
      "{'loss': 3.1665, 'learning_rate': 7.913214697031617e-07, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 8455.5649, 'train_samples_per_second': 25.939, 'train_steps_per_second': 3.242, 'train_loss': 3.262228260095242, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=27417, training_loss=3.262228260095242, metrics={'train_runtime': 8455.5649, 'train_samples_per_second': 25.939, 'train_steps_per_second': 3.242, 'train_loss': 3.262228260095242, 'epoch': 1.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../CLIP-GPT2/models/gpt2-large\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "\n",
    "m_loc = \"../CLIP-GPT2/models/gpt2-large-AVA/checkpoint-27417\"\n",
    "model = GPT2LMHeadModel.from_pretrained(m_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"I like this image, but ive never seen one like this before. great work. good idea, but i think it would have been better with a different background. i don't really like the white background. i like the idea, but the white background is a little distracting.\"},\n",
       " {'generated_text': 'I like this image, but ive never seen it done before. i like the idea, but i think the lighting is a little flat. i like the idea, but the lighting is a bit flat. i like the idea, but i think the lighting is a little flat.'},\n",
       " {'generated_text': \"I like this image, but ive never seen it before. it's a very interesting image. i think it would have been better if you had cropped out the bright light on the right side of the image. this is a really cool photo. i love the colors and the lighting\"},\n",
       " {'generated_text': \"I like this image, but ive never seen it done before. i'm not sure how you did it, but i like it. i hope you explain how you did it. i'm not sure how you did it, but i like it. i hope you explain how you\"},\n",
       " {'generated_text': 'I like this image, but ive never seen one like it before. great job. i like the contrast between the red and the blue, but i think it would have been better if it was a little sharper. i like the composition, but the focus seems a bit soft.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = \"I like this image, but \"\n",
    "tokens = tokenizer.encode(prefix)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "output = pipe(prefix, max_new_tokens=50, num_return_sequences=5, pad_token_id=50256, num_beams=3)\n",
    "\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a885d5bfa55cca12b0dd162160ddb2cadd7a03a3570ec17fb5426c28a2279b3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
