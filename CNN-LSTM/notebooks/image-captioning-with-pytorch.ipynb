{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7169019f",
   "metadata": {
    "papermill": {
     "duration": 0.01026,
     "end_time": "2022-07-20T03:46:46.676208",
     "exception": false,
     "start_time": "2022-07-20T03:46:46.665948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 0. Import library, and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f7ac09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:46.696703Z",
     "iopub.status.busy": "2022-07-20T03:46:46.695820Z",
     "iopub.status.idle": "2022-07-20T03:46:49.712199Z",
     "shell.execute_reply": "2022-07-20T03:46:49.711191Z"
    },
    "papermill": {
     "duration": 3.029524,
     "end_time": "2022-07-20T03:46:49.714726",
     "exception": false,
     "start_time": "2022-07-20T03:46:46.685202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "import torchvision\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c45d3f",
   "metadata": {
    "papermill": {
     "duration": 0.008914,
     "end_time": "2022-07-20T03:46:49.733251",
     "exception": false,
     "start_time": "2022-07-20T03:46:49.724337",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Define some constants for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f08e04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:49.752754Z",
     "iopub.status.busy": "2022-07-20T03:46:49.751946Z",
     "iopub.status.idle": "2022-07-20T03:46:49.758302Z",
     "shell.execute_reply": "2022-07-20T03:46:49.757337Z"
    },
    "papermill": {
     "duration": 0.018328,
     "end_time": "2022-07-20T03:46:49.760291",
     "exception": false,
     "start_time": "2022-07-20T03:46:49.741963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_IMAGES_DIR = \"../datasets/flickr30k/images/\"\n",
    "LABEL_PATH = \"../datasets/flickr30k/captions.txt\"\n",
    "OUTPUT_PATH = \"../CNN-LSTM/working\"\n",
    "\n",
    "# Some special tokens for RNN model...\n",
    "UNK = \"#UNK\"\n",
    "PAD = \"#PAD\"\n",
    "START = \"#START\"\n",
    "END = \"#END\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3df4c02",
   "metadata": {
    "papermill": {
     "duration": 0.008766,
     "end_time": "2022-07-20T03:46:49.778472",
     "exception": false,
     "start_time": "2022-07-20T03:46:49.769706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Read csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a59a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:49.798718Z",
     "iopub.status.busy": "2022-07-20T03:46:49.797345Z",
     "iopub.status.idle": "2022-07-20T03:46:50.191545Z",
     "shell.execute_reply": "2022-07-20T03:46:50.190438Z"
    },
    "papermill": {
     "duration": 0.406648,
     "end_time": "2022-07-20T03:46:50.194164",
     "exception": false,
     "start_time": "2022-07-20T03:46:49.787516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(LABEL_PATH, sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcbd4cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:50.215164Z",
     "iopub.status.busy": "2022-07-20T03:46:50.214822Z",
     "iopub.status.idle": "2022-07-20T03:46:50.233525Z",
     "shell.execute_reply": "2022-07-20T03:46:50.232547Z"
    },
    "papermill": {
     "duration": 0.03163,
     "end_time": "2022-07-20T03:46:50.235535",
     "exception": false,
     "start_time": "2022-07-20T03:46:50.203905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9db6b5",
   "metadata": {
    "papermill": {
     "duration": 0.00947,
     "end_time": "2022-07-20T03:46:50.254247",
     "exception": false,
     "start_time": "2022-07-20T03:46:50.244777",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1. Data visualization & analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de451f8",
   "metadata": {
    "papermill": {
     "duration": 0.008927,
     "end_time": "2022-07-20T03:46:50.272048",
     "exception": false,
     "start_time": "2022-07-20T03:46:50.263121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Preprocess raw text to:\n",
    "1. lower case\n",
    "2. remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8f123b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:50.292134Z",
     "iopub.status.busy": "2022-07-20T03:46:50.290797Z",
     "iopub.status.idle": "2022-07-20T03:46:50.297526Z",
     "shell.execute_reply": "2022-07-20T03:46:50.296723Z"
    },
    "papermill": {
     "duration": 0.018629,
     "end_time": "2022-07-20T03:46:50.299465",
     "exception": false,
     "start_time": "2022-07-20T03:46:50.280836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "def clean_text(row):\n",
    "    row = str(row).strip()\n",
    "    row = row.lower()\n",
    "    return regex.sub(\"\", row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa68bd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:50.318531Z",
     "iopub.status.busy": "2022-07-20T03:46:50.318246Z",
     "iopub.status.idle": "2022-07-20T03:46:50.706242Z",
     "shell.execute_reply": "2022-07-20T03:46:50.705240Z"
    },
    "papermill": {
     "duration": 0.400431,
     "end_time": "2022-07-20T03:46:50.708604",
     "exception": false,
     "start_time": "2022-07-20T03:46:50.308173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.columns = [col.strip() for col in df.columns]\n",
    "df[\"caption_text\"] = df[\"caption_text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880b1a98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T07:19:31.811114Z",
     "iopub.status.busy": "2022-07-04T07:19:31.810722Z",
     "iopub.status.idle": "2022-07-04T07:19:31.818899Z",
     "shell.execute_reply": "2022-07-04T07:19:31.817062Z",
     "shell.execute_reply.started": "2022-07-04T07:19:31.811082Z"
    },
    "papermill": {
     "duration": 0.008779,
     "end_time": "2022-07-20T03:46:50.726917",
     "exception": false,
     "start_time": "2022-07-20T03:46:50.718138",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Compute the length of each sentence and attach it to current dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa119a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:50.746430Z",
     "iopub.status.busy": "2022-07-20T03:46:50.746123Z",
     "iopub.status.idle": "2022-07-20T03:46:50.974596Z",
     "shell.execute_reply": "2022-07-20T03:46:50.973652Z"
    },
    "papermill": {
     "duration": 0.24141,
     "end_time": "2022-07-20T03:46:50.977577",
     "exception": false,
     "start_time": "2022-07-20T03:46:50.736167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"length\"] = df[\"caption_text\"].apply(lambda row: len(row.strip().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb48104",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:50.997291Z",
     "iopub.status.busy": "2022-07-20T03:46:50.996987Z",
     "iopub.status.idle": "2022-07-20T03:46:51.007011Z",
     "shell.execute_reply": "2022-07-20T03:46:51.005939Z"
    },
    "papermill": {
     "duration": 0.023599,
     "end_time": "2022-07-20T03:46:51.010409",
     "exception": false,
     "start_time": "2022-07-20T03:46:50.986810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835c8ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:51.030758Z",
     "iopub.status.busy": "2022-07-20T03:46:51.030444Z",
     "iopub.status.idle": "2022-07-20T03:46:52.219895Z",
     "shell.execute_reply": "2022-07-20T03:46:52.218859Z"
    },
    "papermill": {
     "duration": 1.202277,
     "end_time": "2022-07-20T03:46:52.222116",
     "exception": false,
     "start_time": "2022-07-20T03:46:51.019839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='length', palette='mako', kind='kde', fill=True, aspect=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc47672",
   "metadata": {
    "papermill": {
     "duration": 0.009394,
     "end_time": "2022-07-20T03:46:52.241236",
     "exception": false,
     "start_time": "2022-07-20T03:46:52.231842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Observing that, almost sentence length is <= 30. So I think 30 is good choice as `max_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93adc9e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:52.263287Z",
     "iopub.status.busy": "2022-07-20T03:46:52.262210Z",
     "iopub.status.idle": "2022-07-20T03:46:52.270181Z",
     "shell.execute_reply": "2022-07-20T03:46:52.269309Z"
    },
    "papermill": {
     "duration": 0.021281,
     "end_time": "2022-07-20T03:46:52.272190",
     "exception": false,
     "start_time": "2022-07-20T03:46:52.250909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "captions = df[\"caption_text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e4ce2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:52.292870Z",
     "iopub.status.busy": "2022-07-20T03:46:52.292084Z",
     "iopub.status.idle": "2022-07-20T03:46:52.298633Z",
     "shell.execute_reply": "2022-07-20T03:46:52.297673Z"
    },
    "papermill": {
     "duration": 0.018995,
     "end_time": "2022-07-20T03:46:52.300610",
     "exception": false,
     "start_time": "2022-07-20T03:46:52.281615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "captions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509eadb2",
   "metadata": {
    "papermill": {
     "duration": 0.010115,
     "end_time": "2022-07-20T03:46:52.320689",
     "exception": false,
     "start_time": "2022-07-20T03:46:52.310574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Do some statistic to count the occurence frequency of words in our captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d742f38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:52.341610Z",
     "iopub.status.busy": "2022-07-20T03:46:52.341278Z",
     "iopub.status.idle": "2022-07-20T03:46:53.027450Z",
     "shell.execute_reply": "2022-07-20T03:46:53.026287Z"
    },
    "papermill": {
     "duration": 0.700116,
     "end_time": "2022-07-20T03:46:53.030392",
     "exception": false,
     "start_time": "2022-07-20T03:46:52.330276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_freq = {}\n",
    "for caption in captions:\n",
    "    caption = caption.strip()\n",
    "    for word in caption.split():\n",
    "        if word not in word_freq:\n",
    "            word_freq[word] = 0\n",
    "        word_freq[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4034fd6d",
   "metadata": {
    "papermill": {
     "duration": 0.009573,
     "end_time": "2022-07-20T03:46:53.049851",
     "exception": false,
     "start_time": "2022-07-20T03:46:53.040278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "See the top 30 words appear the most and the least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5be590",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:53.070742Z",
     "iopub.status.busy": "2022-07-20T03:46:53.070414Z",
     "iopub.status.idle": "2022-07-20T03:46:53.090848Z",
     "shell.execute_reply": "2022-07-20T03:46:53.089951Z"
    },
    "papermill": {
     "duration": 0.0333,
     "end_time": "2022-07-20T03:46:53.092763",
     "exception": false,
     "start_time": "2022-07-20T03:46:53.059463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict(sorted(word_freq.items(), key=lambda item: item[1])[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751402f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:53.115295Z",
     "iopub.status.busy": "2022-07-20T03:46:53.113594Z",
     "iopub.status.idle": "2022-07-20T03:46:53.128340Z",
     "shell.execute_reply": "2022-07-20T03:46:53.127455Z"
    },
    "papermill": {
     "duration": 0.02764,
     "end_time": "2022-07-20T03:46:53.130305",
     "exception": false,
     "start_time": "2022-07-20T03:46:53.102665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict(sorted(word_freq.items(), key=lambda item: item[1], reverse=True)[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8711c4c2",
   "metadata": {
    "papermill": {
     "duration": 0.01029,
     "end_time": "2022-07-20T03:46:53.150512",
     "exception": false,
     "start_time": "2022-07-20T03:46:53.140222",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8511eef",
   "metadata": {
    "papermill": {
     "duration": 0.010522,
     "end_time": "2022-07-20T03:46:53.171162",
     "exception": false,
     "start_time": "2022-07-20T03:46:53.160640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "By having preprocessed captions, we start to build vocabulary in our dataset and convert string to token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77828a43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:53.192231Z",
     "iopub.status.busy": "2022-07-20T03:46:53.191907Z",
     "iopub.status.idle": "2022-07-20T03:46:53.199685Z",
     "shell.execute_reply": "2022-07-20T03:46:53.198660Z"
    },
    "papermill": {
     "duration": 0.020721,
     "end_time": "2022-07-20T03:46:53.201697",
     "exception": false,
     "start_time": "2022-07-20T03:46:53.180976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_vocab(captions, word_freq, count_threshold=5):\n",
    "    \"\"\"\n",
    "    This function builds `vocab` dictionary from list of text captions.\n",
    "    Also, add constant PAD, UNK, START, END to `vocab`.\n",
    "    Add a word to vocab if its occurence frequency is larger than `count_threshold`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    captions: a list of preprocessed text captions above.\n",
    "    word_freq: a dictionary of word occurence frequency.\n",
    "    count_threshold: a int to use when building vocab.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocab: an dictionary vocabulary of key-value pair which is:\n",
    "        -> key: string text\n",
    "        -> value:  token index\n",
    "    inv_vocab: an inverse dictionary vocabulary of key-value pair which is:\n",
    "        -> key: token index\n",
    "        -> value: string text\n",
    "        \n",
    "    E.g: vocab = {\"two\": 4, \"young\": 5, \"guys\": 6, ...} \n",
    "         inv_vocab = {4: \"two\", 5: \"young\", 6: \"guys\", ...}\n",
    "    \"\"\"\n",
    "    vocab = {\n",
    "        PAD: 0,\n",
    "        UNK: 1,\n",
    "        START: 2,\n",
    "        END: 3\n",
    "    }\n",
    "    index = 4\n",
    "    \n",
    "    for caption in captions:\n",
    "        caption = caption.strip().split(\" \")\n",
    "        for word in caption:\n",
    "            if word and word_freq[word] >= count_threshold and word not in vocab:\n",
    "                vocab[word] = index\n",
    "                index += 1\n",
    "\n",
    "    inv_vocab = {v: k for k, v in vocab.items()}\n",
    "    return vocab, inv_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8507a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:53.223814Z",
     "iopub.status.busy": "2022-07-20T03:46:53.222989Z",
     "iopub.status.idle": "2022-07-20T03:46:53.732901Z",
     "shell.execute_reply": "2022-07-20T03:46:53.731923Z"
    },
    "papermill": {
     "duration": 0.523027,
     "end_time": "2022-07-20T03:46:53.735258",
     "exception": false,
     "start_time": "2022-07-20T03:46:53.212231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab, inv_vocab = build_vocab(captions, word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe9881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:53.757213Z",
     "iopub.status.busy": "2022-07-20T03:46:53.756862Z",
     "iopub.status.idle": "2022-07-20T03:46:53.765844Z",
     "shell.execute_reply": "2022-07-20T03:46:53.763954Z"
    },
    "papermill": {
     "duration": 0.022562,
     "end_time": "2022-07-20T03:46:53.767997",
     "exception": false,
     "start_time": "2022-07-20T03:46:53.745435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_captions(captions, vocab, max_length=30):\n",
    "    \"\"\"\n",
    "    Convert text captions to index token based on `vocab`.\n",
    "    If a word not in vocab, replace it by the token index of `UNK` constant.\n",
    "    Also, add `START` constant to the beginning of the sentence and \n",
    "            `END` constant to the end of the sentence.\n",
    "    After add `START` and `END` constant, if the length is still < 30,\n",
    "        use `PAD` constant to fill remaining positions.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    captions: a list of preprocessed text captions above.\n",
    "    vocab: a dictionary vocabulary of key-value pair which is:\n",
    "        -> key: string text\n",
    "        -> value: token index\n",
    "    max_length: an int denotes fixed maximum length to the captions.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tokens: a list of tokens get from `vocab`\n",
    "    \"\"\"\n",
    "    tokens = [[vocab[PAD]]*max_length for _ in range(len(captions))]\n",
    "    for i, caption in enumerate(captions):\n",
    "        caption = caption.strip().split()\n",
    "        tokens[i][0] = vocab[START]\n",
    "        j = 1\n",
    "        for word in caption[:max_length-2]:\n",
    "            if word not in vocab:\n",
    "                tokens[i][j] = vocab[UNK]\n",
    "            else:\n",
    "                tokens[i][j] = vocab[word]\n",
    "            j += 1\n",
    "        tokens[i][j] = vocab[END]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd032b44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:53.789668Z",
     "iopub.status.busy": "2022-07-20T03:46:53.789362Z",
     "iopub.status.idle": "2022-07-20T03:46:54.942795Z",
     "shell.execute_reply": "2022-07-20T03:46:54.941655Z"
    },
    "papermill": {
     "duration": 1.167553,
     "end_time": "2022-07-20T03:46:54.945629",
     "exception": false,
     "start_time": "2022-07-20T03:46:53.778076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = convert_captions(captions, vocab)\n",
    "img_paths = list(df[\"image_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d48ad7e",
   "metadata": {
    "papermill": {
     "duration": 0.016847,
     "end_time": "2022-07-20T03:46:54.972865",
     "exception": false,
     "start_time": "2022-07-20T03:46:54.956018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Define pytorch Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95bab4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:54.995868Z",
     "iopub.status.busy": "2022-07-20T03:46:54.994953Z",
     "iopub.status.idle": "2022-07-20T03:46:55.008342Z",
     "shell.execute_reply": "2022-07-20T03:46:55.007351Z"
    },
    "papermill": {
     "duration": 0.027082,
     "end_time": "2022-07-20T03:46:55.010726",
     "exception": false,
     "start_time": "2022-07-20T03:46:54.983644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageCaptioningDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, img_paths, tokens):\n",
    "        \"\"\"\n",
    "        img_paths: a list of image path we get from dataframe\n",
    "        tokens: a list of tokens that we converted from text captions\n",
    "        \"\"\"\n",
    "        self.img_paths = [os.path.join(INPUT_IMAGES_DIR, p) for p in img_paths]\n",
    "        self.tokens = tokens\n",
    "        assert len(self.img_paths) == len(self.tokens), \"Make sure len(img_paths) == len(tokens).\"\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get image path and token. Then load image path to numpy array image. Convert to pytorch tensor if it's necessary. \n",
    "        \"\"\"\n",
    "        img_path = self.img_paths[index]\n",
    "        token = self.tokens[index]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = self._resize_img(img, shape=(300, 300))\n",
    "        img = torchvision.transforms.ToTensor()(img)\n",
    "        token = torch.as_tensor(token)\n",
    "        return img, token\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def _resize_img(self, img, shape=(300, 300)):\n",
    "        h, w = img.shape[0], img.shape[1]\n",
    "        pad_left = 0\n",
    "        pad_right = 0\n",
    "        pad_top = 0\n",
    "        pad_bottom = 0\n",
    "        if h > w:\n",
    "            diff = h - w\n",
    "            pad_top = diff - diff // 2\n",
    "            pad_bottom = diff // 2\n",
    "        else:\n",
    "            diff = w - h\n",
    "            pad_left = diff - diff // 2\n",
    "            pad_right = diff // 2\n",
    "        cropped_img = img[pad_top:h-pad_bottom, pad_left:w-pad_right, :]\n",
    "        cropped_img = cv2.resize(cropped_img, shape)\n",
    "        return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d859844",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:55.033034Z",
     "iopub.status.busy": "2022-07-20T03:46:55.032713Z",
     "iopub.status.idle": "2022-07-20T03:46:55.284048Z",
     "shell.execute_reply": "2022-07-20T03:46:55.283062Z"
    },
    "papermill": {
     "duration": 0.264918,
     "end_time": "2022-07-20T03:46:55.286440",
     "exception": false,
     "start_time": "2022-07-20T03:46:55.021522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = ImageCaptioningDataset(img_paths, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4b47a4",
   "metadata": {
    "papermill": {
     "duration": 0.010343,
     "end_time": "2022-07-20T03:46:55.307112",
     "exception": false,
     "start_time": "2022-07-20T03:46:55.296769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 3. Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b7ebdf",
   "metadata": {
    "papermill": {
     "duration": 0.009678,
     "end_time": "2022-07-20T03:46:55.326807",
     "exception": false,
     "start_time": "2022-07-20T03:46:55.317129",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A picture is better than thoudsand words:\n",
    "![Image Captioning Model](https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/03-advanced/image_captioning/png/model.png)\n",
    "\n",
    "In this architecture, the encoder is CNN which outputs a feature vector. Then, the decoder RNN (LSTM) uses this feature vector as initial hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d7a12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:55.347980Z",
     "iopub.status.busy": "2022-07-20T03:46:55.347622Z",
     "iopub.status.idle": "2022-07-20T03:46:55.412380Z",
     "shell.execute_reply": "2022-07-20T03:46:55.411422Z"
    },
    "papermill": {
     "duration": 0.077765,
     "end_time": "2022-07-20T03:46:55.414366",
     "exception": false,
     "start_time": "2022-07-20T03:46:55.336601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 30\n",
    "NUM_VOCAB = len(vocab)\n",
    "BATCH_SIZE = 128\n",
    "EPOCH = 5\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51607c4",
   "metadata": {
    "papermill": {
     "duration": 0.009779,
     "end_time": "2022-07-20T03:46:55.434217",
     "exception": false,
     "start_time": "2022-07-20T03:46:55.424438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### 3.1 Define CNN encoder class:\n",
    "\n",
    "The best practice is to use pretrained models from ImageNet: VGG, Resnet, Alexnet, Googlenet,... We can call those pretrained models are the backbones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff464fb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:55.455971Z",
     "iopub.status.busy": "2022-07-20T03:46:55.455096Z",
     "iopub.status.idle": "2022-07-20T03:46:55.461065Z",
     "shell.execute_reply": "2022-07-20T03:46:55.460212Z"
    },
    "papermill": {
     "duration": 0.019161,
     "end_time": "2022-07-20T03:46:55.463195",
     "exception": false,
     "start_time": "2022-07-20T03:46:55.444034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.cnn = torchvision.models.resnet34(pretrained=True)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.cnn(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f731d88",
   "metadata": {
    "papermill": {
     "duration": 0.009677,
     "end_time": "2022-07-20T03:46:55.483175",
     "exception": false,
     "start_time": "2022-07-20T03:46:55.473498",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### 3.2 Define LSTM decoder class:\n",
    "\n",
    "In this class, you should have to define nn.Embedding, nn.LSTM, nn.Linear,... to appropriate training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ecbbc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:55.504300Z",
     "iopub.status.busy": "2022-07-20T03:46:55.503710Z",
     "iopub.status.idle": "2022-07-20T03:46:55.521343Z",
     "shell.execute_reply": "2022-07-20T03:46:55.520493Z"
    },
    "papermill": {
     "duration": 0.030378,
     "end_time": "2022-07-20T03:46:55.523310",
     "exception": false,
     "start_time": "2022-07-20T03:46:55.492932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNNDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_vocab) -> None:\n",
    "        super().__init__()\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.num_vocab = num_vocab\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_vocab, embedding_dim=256, padding_idx=0)\n",
    "        self.num_layers = 1\n",
    "        self.bidirectional = False\n",
    "        self.rnn = nn.LSTM(input_size=256, hidden_size=256, num_layers=self.num_layers, batch_first=False, bidirectional=self.bidirectional)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, num_vocab)\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input, img_embeded, prediction=False):\n",
    "        img_embeded = self.bottleneck(img_embeded)\n",
    "        img_embeded = torch.stack([img_embeded]*(self.num_layers), dim=0)\n",
    "        if prediction:\n",
    "            output = []\n",
    "            hidden = (img_embeded, img_embeded)\n",
    "            out = input\n",
    "            while out != vocab[END] and len(output) <= MAX_LENGTH:\n",
    "                out = torch.tensor([[out]]).to(DEVICE)\n",
    "                out = self.embedding(out)\n",
    "                out = out.permute(1, 0, 2)\n",
    "                out, hidden = self.rnn(out, hidden)\n",
    "                out = out.permute(1, 0, 2)\n",
    "                out = self.classifier(out)\n",
    "                out = self.softmax(out)\n",
    "                out = torch.argmax(out, dim=-1)\n",
    "                out = out.squeeze().item()\n",
    "                output.append(out)\n",
    "        else:\n",
    "            input = self.embedding(input)\n",
    "            input = input.permute(1, 0, 2)\n",
    "            output, (h, c) = self.rnn(input, (img_embeded, img_embeded))\n",
    "            output = output.permute(1, 0, 2)\n",
    "            output = self.classifier(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acfb651",
   "metadata": {
    "papermill": {
     "duration": 0.009661,
     "end_time": "2022-07-20T03:46:55.542868",
     "exception": false,
     "start_time": "2022-07-20T03:46:55.533207",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 4. Train model\n",
    "\n",
    "In this part, you should combine what you defined to train model (Dataset, Encoder, Decoder,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12f83f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:55.564230Z",
     "iopub.status.busy": "2022-07-20T03:46:55.563397Z",
     "iopub.status.idle": "2022-07-20T03:46:55.577410Z",
     "shell.execute_reply": "2022-07-20T03:46:55.576416Z"
    },
    "papermill": {
     "duration": 0.026735,
     "end_time": "2022-07-20T03:46:55.579329",
     "exception": false,
     "start_time": "2022-07-20T03:46:55.552594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageCaptioningModel:\n",
    "\n",
    "    def __init__(self, encoder : CNNEncoder, decoder : RNNDecoder, train_dataset : ImageCaptioningDataset):\n",
    "        self.encoder = encoder.to(DEVICE)\n",
    "        self.encoder.eval()\n",
    "        self.decoder = decoder.to(DEVICE)\n",
    "        self.train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        self.optimizer = optim.Adam(decoder.parameters())\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def predict(self, img):\n",
    "        with torch.no_grad():\n",
    "            img_embed = self.encoder(img)\n",
    "            caption = vocab[START]\n",
    "            caption = self.decoder(caption, img_embed, prediction=True)\n",
    "        \n",
    "        text = [inv_vocab[t] for t in caption]\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "    \n",
    "    def train(self):\n",
    "        for e in range(EPOCH):\n",
    "            pbar = tqdm(self.train_dataloader, desc=\"Epoch: {}\".format(e+1))\n",
    "            for i, (img, caption) in enumerate(pbar):\n",
    "                img = img.to(DEVICE)\n",
    "                caption = caption.to(DEVICE)\n",
    "                img_embed = self.encoder(img)\n",
    "                output = self.decoder(caption[:, :-1], img_embed)\n",
    "                output = output.permute(0, 2, 1)\n",
    "                loss = self.loss(output, caption[:, 1:])\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward() \n",
    "                self.optimizer.step()\n",
    "\n",
    "                pbar.set_description(desc=\"Epoch \" + str(e+1) + \" - Loss: %.5f\" % (loss.item()))\n",
    "                \n",
    "                if ((i+1)%100) == 0:\n",
    "                    plt.imshow(img[-1].cpu().detach().numpy().transpose((1, 2, 0)))\n",
    "                    output = self.predict(img[-1].unsqueeze(0))\n",
    "                    plt.title(output)\n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0403f34a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:46:55.600545Z",
     "iopub.status.busy": "2022-07-20T03:46:55.599771Z",
     "iopub.status.idle": "2022-07-20T03:47:04.514806Z",
     "shell.execute_reply": "2022-07-20T03:47:04.513851Z"
    },
    "papermill": {
     "duration": 8.928252,
     "end_time": "2022-07-20T03:47:04.517348",
     "exception": false,
     "start_time": "2022-07-20T03:46:55.589096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn = CNNEncoder()\n",
    "rnn = RNNDecoder(num_vocab=NUM_VOCAB)\n",
    "model = ImageCaptioningModel(encoder=cnn, decoder=rnn, train_dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790d45d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(self, img, shape=(300, 300)):\n",
    "        h, w = img.shape[0], img.shape[1]\n",
    "        pad_left = 0\n",
    "        pad_right = 0\n",
    "        pad_top = 0\n",
    "        pad_bottom = 0\n",
    "        if h > w:\n",
    "            diff = h - w\n",
    "            pad_top = diff - diff // 2\n",
    "            pad_bottom = diff // 2\n",
    "        else:\n",
    "            diff = w - h\n",
    "            pad_left = diff - diff // 2\n",
    "            pad_right = diff // 2\n",
    "        cropped_img = img[pad_top:h-pad_bottom, pad_left:w-pad_right, :]\n",
    "        cropped_img = cv2.resize(cropped_img, shape)\n",
    "        return cropped_img\n",
    "\n",
    "img_path = \"../datasets/flickr30k/test_examples/boat.png\"\n",
    "img = cv2.imread(img_path)\n",
    "img = resize_img(img, shape=(300, 300))\n",
    "img = torchvision.transforms.ToTensor()(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a60239c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T03:47:04.540054Z",
     "iopub.status.busy": "2022-07-20T03:47:04.539695Z",
     "iopub.status.idle": "2022-07-20T06:36:36.154603Z",
     "shell.execute_reply": "2022-07-20T06:36:36.153549Z"
    },
    "papermill": {
     "duration": 10171.629699,
     "end_time": "2022-07-20T06:36:36.157480",
     "exception": false,
     "start_time": "2022-07-20T03:47:04.527781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5f482e",
   "metadata": {
    "papermill": {
     "duration": 0.919878,
     "end_time": "2022-07-20T06:36:37.940548",
     "exception": false,
     "start_time": "2022-07-20T06:36:37.020670",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 5. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905b1aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T06:36:39.640254Z",
     "iopub.status.busy": "2022-07-20T06:36:39.639699Z",
     "iopub.status.idle": "2022-07-20T06:36:39.644364Z",
     "shell.execute_reply": "2022-07-20T06:36:39.643288Z"
    },
    "papermill": {
     "duration": 0.863887,
     "end_time": "2022-07-20T06:36:39.646915",
     "exception": false,
     "start_time": "2022-07-20T06:36:38.783028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO 7: predict on images after you trained model...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10203.475876,
   "end_time": "2022-07-20T06:36:42.035735",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-20T03:46:38.559859",
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "06179fa2b8cfafd36062f042ab1b5d69dd100e6b8d44f3b802d9de85bc5b3b99"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "09bb2dc08f5f4c7e9e860bc8d3f97002": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "15b4e2e5e5fb4c509a3d9657bffcd551": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1d0e2dfac48f4badae5d7270d33f90e0",
       "max": 87319819,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_914b4b3e919b47b3b05a041b4437e669",
       "value": 87319819
      }
     },
     "1d0e2dfac48f4badae5d7270d33f90e0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4f3373f44fd54a169e0492c5d942afbb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6b449f23384e42b48d7976206dc9cd6a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_09bb2dc08f5f4c7e9e860bc8d3f97002",
       "placeholder": "​",
       "style": "IPY_MODEL_c91359434f5c43c2bc06ffe694512c64",
       "value": " 83.3M/83.3M [00:04&lt;00:00, 25.0MB/s]"
      }
     },
     "6d275812cc58471fb7dcf09e2db350cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4f3373f44fd54a169e0492c5d942afbb",
       "placeholder": "​",
       "style": "IPY_MODEL_6f64a92c95474d65af26d0493e6a69b8",
       "value": "100%"
      }
     },
     "6f64a92c95474d65af26d0493e6a69b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "914b4b3e919b47b3b05a041b4437e669": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c823b6f25ee543a9ae22938dc86e4d4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6d275812cc58471fb7dcf09e2db350cf",
        "IPY_MODEL_15b4e2e5e5fb4c509a3d9657bffcd551",
        "IPY_MODEL_6b449f23384e42b48d7976206dc9cd6a"
       ],
       "layout": "IPY_MODEL_fccf47775a9c406ea5ec7fddb0770dd3"
      }
     },
     "c91359434f5c43c2bc06ffe694512c64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fccf47775a9c406ea5ec7fddb0770dd3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
