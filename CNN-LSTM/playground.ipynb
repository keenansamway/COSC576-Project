{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import spacy\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from utils import print_examples, save_checkpoint, load_checkpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this is very\"\n",
    "words = text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[482,  17, 371]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[dataset.vocab.stoi[w] for w in words[0:]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# class Vocabulary:\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        \n",
    "        ## REMOVE USELESS WORDS AND CHARACTERS\n",
    "        \n",
    "        tokenized_text = [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "        return tokenized_text\n",
    "    \n",
    "    # Create dictionary of vocabulary and frequency\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                if word not in frequencies:\n",
    "                    # add word to frequencies dictionary, set frequency to 1 (initial word)\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    # word is in frequencies dictionary, increment frequency by 1\n",
    "                    frequencies[word] += 1\n",
    "                    \n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    # word has reached frequency threshold in vocab dictionary\n",
    "                    # add word to vocab dictionary (at most once)\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "                    \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "        \n",
    "        return [self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flickr8k Dataset\n",
    "class Flickr8k(Dataset):\n",
    "    def __init__(self, imgs_dir, captions_file, test_file, transform=None, freq_threshold=5):\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.df = pd.read_csv(captions_file)\n",
    "        self.test_file = test_file\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.imgs = self.df['image']\n",
    "        self.captions = self.df['caption']\n",
    "        \n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.captions.tolist())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        caption = self.captions[index]\n",
    "        img_id = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.imgs_dir, img_id)).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "        \n",
    "        return img, torch.tensor(numericalized_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom collate\n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        # Sort batch list by caption length in descending order (longest to shortest)\n",
    "        batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "        \n",
    "        ## TEST IF SORTING IS NEEDED OR NOT\n",
    "        \n",
    "        imgs, captions = zip(*batch)\n",
    "            \n",
    "        imgs = torch.stack(imgs)\n",
    "        lengths = [len(cap) for cap in captions]\n",
    "        targets = pad_sequence(captions, batch_first=False, padding_value=self.pad_idx)\n",
    "        \"\"\"\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "        captions = [item[1] for item in batch]\n",
    "        lengths = [len(cap) for cap in captions]\n",
    "        targets = pad_sequence(captions, batch_first=False, padding_value=self.pad_idx)\n",
    "        \"\"\"\n",
    "        # imgs:    (batch size, 3, 224, 224)\n",
    "        # targets: (sequence length, batch size)\n",
    "        # lengths: (batch size)\n",
    "        return imgs, targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_loader()\n",
    "def get_loader(dataset_to_use, imgs_folder, annotation_file, transform, test_file=\"\", batch_size=32, num_workers=8, freq_threshold=5, shuffle=True, pin_memory=True):\n",
    "    \n",
    "    #if dataset_to_use == \"PCCD\":\n",
    "        #dataset = PCCD(imgs_folder, annotation_file, test_file, transform=transform, freq_threshold=freq_threshold)\n",
    "    if dataset_to_use == \"flickr8k\":\n",
    "        dataset = Flickr8k(imgs_folder, annotation_file, test_file, transform=transform, freq_threshold=freq_threshold)\n",
    "    \n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
    "    )\n",
    "    \n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename):\n",
    "    print(\"-- Saving Checkpoint --\")\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "    \n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"-- Loading Checkpoint --\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    step = checkpoint[\"step\"]\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples(model, device, dataset):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    filename_loc = dataset.test_file\n",
    "    images_loc = dataset.imgs_dir\n",
    "    \n",
    "    filename_list = pd.read_csv(filename_loc, header=None)\n",
    "    filename_list = filename_list.values.reshape(-1).tolist()\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for i, dir in enumerate(filename_list):\n",
    "        path = os.path.join(images_loc, dir)\n",
    "        test_img = transform(Image.open(path).convert(\"RGB\")).unsqueeze(0)\n",
    "        print(f\"Example {i}) OUTPUT: \" + \" \".join(model.caption_image(test_img.to(device), dataset.vocab)))\n",
    "        if i > 5:\n",
    "            break\n",
    "        \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language Model\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, dropout, bidirectional=False):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        if num_layers < 2: dropout=0.0\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Word Embeddings - https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "        # Input:  (sequence length, batch size)\n",
    "        # Output: (sequence length, batch size, embed size)\n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)\n",
    "        \n",
    "        # LSTM - https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "        # Input:  (sequence length, batch size, embed size)\n",
    "        # Output: (sequence length, batch size, 2 if bidirectional else 1, hidden size)\n",
    "        self.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, batch_first=False,bidirectional=bidirectional)\n",
    "        \n",
    "        # Fully Connected\n",
    "        # Input:  (sequence length, batch size, hidden size)\n",
    "        # Output: (sequence length, batch size, vocab size)\n",
    "        self.linear = nn.Linear(in_features=hidden_size, out_features=vocab_size,)\n",
    "    \n",
    "    def forward(self, features, captions, lengths):\n",
    "        \n",
    "        embeddings = self.embed(captions)\n",
    "        \n",
    "        #https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=False)    \n",
    "        \n",
    "        lstm_out, _ = self.lstm(packed)\n",
    "        outputs = self.linear(lstm_out)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def generate_text(self, vocabulary, max_length=50):\n",
    "        result_text = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inputs = vocabulary.stoi[\"<SOS>\"]\n",
    "            inputs = self.embed(inputs)\n",
    "            hidden = None\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                lstm_out, hidden = self.lstm(inputs, hidden)\n",
    "                lstm_out = lstm_out.squeeze(0)\n",
    "                output = self.linear(lstm_out)\n",
    "                \n",
    "                predicted = torch.argmax(output, dim=1)\n",
    "                result_text.append(predicted.item())\n",
    "                \n",
    "                inputs = self.embed(predicted)\n",
    "                inputs = inputs.unsqueeze(0)\n",
    "                \n",
    "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
    "                    break\n",
    "                \n",
    "        return [vocabulary.itos[idx] for idx in result_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 3e-4\n",
    "batch_size = 32\n",
    "num_workers = 2\n",
    "dropout = 0.0\n",
    "\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_to_use = \"PCCD\"\n",
    "dataset_to_use = \"flickr8k\"\n",
    "\n",
    "if dataset_to_use == \"PCCD\":\n",
    "    imgs_folder = \"../datasets/PCCD/images/full\"\n",
    "    annotation_file = \"../datasets/PCCD/raw.json\"\n",
    "    \n",
    "elif dataset_to_use == \"flickr8k\":\n",
    "    imgs_folder = \"../datasets/flickr8k/images\"\n",
    "    annotation_file = \"../datasets/flickr8k/captions.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((356,356)),\n",
    "        transforms.RandomCrop((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_loader, dataset = get_loader(\n",
    "    dataset_to_use=dataset_to_use,\n",
    "    imgs_folder=imgs_folder,\n",
    "    annotation_file=annotation_file,\n",
    "    transform=transform,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    freq_threshold=5,\n",
    ")\n",
    "vocab_size = len(dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")           ## Nvidia CUDA Acceleration\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")    ## Apple M1 Metal Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = DecoderLSTM(\n",
    "    embed_size=embed_size,\n",
    "    hidden_size=hidden_size,\n",
    "    vocab_size=vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = optim.AdamW(language_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "save_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_model:\n",
    "    writer = SummaryWriter(os.path.join(\"CNN_LSTM/runs/language_model\", dataset_to_use))\n",
    "step = 0\n",
    "\n",
    "if load_model:\n",
    "    step = load_checkpoint(torch.load(\"CNN_LSTM/runs/language_model/checkpoint.path.tar\"), language_model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/keenansamway/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/keenansamway/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'Flickr8k' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m language_model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mfor\u001b[39;00m idx, (imgs, captions, lengths) \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39;49m(train_loader), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_loader), leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         captions \u001b[39m=\u001b[39m captions\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:435\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[1;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:381\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 381\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1034\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1027\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m   1035\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1036\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_posix\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(process_obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch(process_obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentinel \u001b[39m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(parent_w, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m, closefd\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[39m.\u001b[39;49mwrite(fp\u001b[39m.\u001b[39;49mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "language_model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for idx, (imgs, captions, lengths) in tqdm(enumerate(train_loader), total=len(train_loader), leave=True):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        captions = captions.to(device)\n",
    "        inputs = None\n",
    "        \n",
    "        outputs = language_model(inputs, captions, lengths)\n",
    "        \n",
    "        captions = captions.reshape(-1)\n",
    "        \n",
    "        outputs = outputs.reshape(-1, outputs.shape[2])\n",
    "        \n",
    "        loss = criterion(outputs, captions)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if save_model:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'step': step,\n",
    "            'loss': loss,\n",
    "            'state_dict': language_model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(checkpoint, filename=\"CNN-LSTM/runs/language_model/checkpoint.pth.tar\")\n",
    "        \n",
    "    print(\"Epoch [{}/[{}], Loss: {:.4f}\".format(epoch+1, num_epochs, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint(torch.load(\"CNN_LSTM/runs/language_model/checkpoint.path.tar\"), language_model, optimizer)\n",
    "\n",
    "print_examples(language_model, device, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06179fa2b8cfafd36062f042ab1b5d69dd100e6b8d44f3b802d9de85bc5b3b99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
