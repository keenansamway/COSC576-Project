{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import spacy, re\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import os, sys, io, json\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# class Vocabulary:\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        \n",
    "        ## REMOVE SPECIAL CHARACTERS\n",
    "        #cleaned_text = ''.join(e for e in text if e.isalnum())\n",
    "        #cleaned_text = re.sub(r\"[^a-zA-Z0-9 ]\", \"\", text)\n",
    "        cleaned_text = text\n",
    "        \n",
    "        tokenized_text = [tok.text.lower() for tok in spacy_eng.tokenizer(cleaned_text)]\n",
    "        return tokenized_text\n",
    "    \n",
    "    # Create dictionary of vocabulary and frequency\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                if word not in frequencies:\n",
    "                    # add word to frequencies dictionary, set frequency to 1 (initial word)\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    # word is in frequencies dictionary, increment frequency by 1\n",
    "                    frequencies[word] += 1\n",
    "                    \n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    # word has reached frequency threshold in vocab dictionary\n",
    "                    # add word to vocab dictionary (at most once)\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "                    \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "        \n",
    "        return [self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flickr8k Dataset\n",
    "class Flickr8k(Dataset):\n",
    "    def __init__(self, imgs_dir, captions_file, test_file, transform=None, freq_threshold=5):\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.df = pd.read_csv(captions_file)\n",
    "        self.test_file = test_file\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.imgs = self.df['image']\n",
    "        self.captions = self.df['caption']\n",
    "        \n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.captions.tolist())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        caption = self.captions[index]\n",
    "        img_id = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.imgs_dir, img_id)).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "        \n",
    "        return img, torch.tensor(numericalized_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCCD Dataset\n",
    "class PCCD(Dataset):\n",
    "    def __init__(self, imgs_dir, captions_file, test_file, transform=None, freq_threshold=5):\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.df = pd.read_json(captions_file)\n",
    "        self.test_file = test_file\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.df[\"general_impression\"] = self.df[\"general_impression\"].fillna(\"\")\n",
    "        \n",
    "        # Get img, caption columns\n",
    "        self.imgs = self.df[\"title\"]\n",
    "        self.captions = self.df[\"general_impression\"]\n",
    "\n",
    "        # Initialize and build vocabulary\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.captions.tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        caption = self.captions[index]\n",
    "        img_id = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.imgs_dir, img_id)).convert(\"RGB\")\n",
    "                \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "        \n",
    "        return img, torch.tensor(numericalized_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVA-Captions Dataset\n",
    "class AVA(Dataset):\n",
    "    def __init__(self, imgs_dir, captions_file, test_file, transform=None, freq_threshold=5):\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.captions_file = captions_file\n",
    "        self.test_file = test_file\n",
    "        self.transform = transform\n",
    "        self.freq_threshold = freq_threshold\n",
    "        \n",
    "        \"\"\"\n",
    "        # Open dataframe\n",
    "        with io.open(self.captions_file, 'r', encoding='utf-8') as f:\n",
    "            json_file = json.load(f)\n",
    "        self.df = pd.DataFrame(json_file['images'])\n",
    "        \n",
    "        #Get img, caption columns\n",
    "        self.imgs = []\n",
    "        self.captions = []\n",
    "        for i, img in enumerate(self.df['filename']):\n",
    "            for j, caption in enumerate(self.df['sentences'][i]):\n",
    "                if os.path.exists(os.path.join(self.imgs_dir, img)):\n",
    "                    self.imgs.append(img)\n",
    "                    self.captions.append(caption['clean'])\n",
    "        \"\"\"\n",
    "        self.df = pd.read_feather(captions_file)\n",
    "        \n",
    "        self.imgs = self.df['filename']\n",
    "        self.captions = self.df['clean_sentence']\n",
    "        self.split = self.df['split']\n",
    "        \n",
    "        # Initialize and build vocab\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.captions)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        caption = self.captions[index]\n",
    "        img_id = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.imgs_dir, img_id)).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "        \n",
    "        return img, torch.tensor(numericalized_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom collate\n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        # Sort batch list by caption length in descending order (longest to shortest)\n",
    "        #batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "        \n",
    "        ## TEST IF SORTING IS NEEDED OR NOT\n",
    "        \n",
    "        imgs = [item[0] for item in batch]\n",
    "        imgs = torch.stack(imgs)\n",
    "        \n",
    "        captions = [item[1] for item in batch]\n",
    "        #truncated = [cap[0:50] if 50 > len(cap) else cap for cap in captions]\n",
    "        targets = pad_sequence(captions, batch_first=False, padding_value=self.pad_idx)\n",
    "        #packed_targets = pack_padded_sequence(targets, batch_first=False, enforce_sorted=False)\n",
    "        \n",
    "        lengths = [len(cap) for cap in captions]\n",
    "\n",
    "        # imgs:    (batch size, 3, 224, 224)\n",
    "        # targets: (sequence length, batch size)\n",
    "        # lengths: (batch size)\n",
    "        return imgs, targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_loader()\n",
    "def get_loader(dataset_to_use, imgs_folder, annotation_file, transform, test_file=\"\", batch_size=32, num_workers=8, freq_threshold=5, shuffle=True, pin_memory=True):\n",
    "    \n",
    "    if dataset_to_use == \"PCCD\":\n",
    "        dataset = PCCD(imgs_folder, annotation_file, test_file, transform=transform, freq_threshold=freq_threshold)\n",
    "    elif dataset_to_use == \"flickr8k\":\n",
    "        dataset = Flickr8k(imgs_folder, annotation_file, test_file, transform=transform, freq_threshold=freq_threshold)\n",
    "    elif dataset_to_use == \"AVA\":\n",
    "        dataset = AVA(imgs_folder, annotation_file, test_file, transform=transform, freq_threshold=freq_threshold)\n",
    "    \n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
    "    )\n",
    "    \n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename):\n",
    "    print(\"-- Saving Checkpoint --\")\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "    \n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"-- Loading Checkpoint --\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    step = checkpoint[\"step\"]\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples(model, device, dataset):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    filename_loc = dataset.test_file\n",
    "    images_loc = dataset.imgs_dir\n",
    "    \n",
    "    filename_list = pd.read_csv(filename_loc, header=None)\n",
    "    filename_list = filename_list.values.reshape(-1).tolist()\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for i, dir in enumerate(filename_list):\n",
    "        path = os.path.join(images_loc, dir)\n",
    "        test_img = transform(Image.open(path).convert(\"RGB\")).unsqueeze(0)\n",
    "        print(f\"Example {i}) OUTPUT: \" + \" \".join(model.caption_image(test_img.to(device), dataset.vocab)))\n",
    "        if i > 5:\n",
    "            break\n",
    "        \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EncoderCNN(nn.Module):\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size, dropout, train_model=False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.train_model = train_model\n",
    "        \n",
    "        self.resnet = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.in_features = self.resnet.fc.in_features\n",
    "        \n",
    "        self.resnet = nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
    "        self.linear = nn.Linear(self.in_features, embed_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        # images: (batch_size, 3, 224, 224)\n",
    "        \n",
    "        features = self.resnet(images)                      # features: (batch_size, 2048, 1, 1)\n",
    "        features = features.view(features.size(0), -1)      # features: (batch_size, 2048)\n",
    "        features = self.linear(features)                    # features: (batch_size, embed_size)\n",
    "        #features = self.relu(features)                      \n",
    "        # features = self.bn(features)\n",
    "        features = self.dropout(features)                   # features: (batch_size, embed_size)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language Model\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, dropout, bidirectional=False):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        if num_layers < 2: dropout=0.0\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Word Embeddings - https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "        # Input:  (sequence length, batch size)\n",
    "        # Output: (sequence length, batch size, embed size)\n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=vocab_size, \n",
    "            embedding_dim=embed_size,\n",
    "            )\n",
    "        \n",
    "        # LSTM - https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "        # Input:  (sequence length, batch size, embed size)\n",
    "        # States: (2 if bidirectional else 1 * num layers, batch size, hidden size)\n",
    "        # Output: (sequence length, batch size, 2 if bidirectional else 1 * hidden size)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_size,\n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            dropout=dropout, \n",
    "            batch_first=False,\n",
    "            bidirectional=bidirectional,\n",
    "            )\n",
    "        \n",
    "        # Fully Connected\n",
    "        # Input:  (sequence length, batch size, hidden size)\n",
    "        # Output: (sequence length, batch size, vocab size)\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=hidden_size, \n",
    "            out_features=vocab_size,\n",
    "            )\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        # features: (batch_size, embed_size)\n",
    "        # captions: (caption_length, batch_size)\n",
    "        \n",
    "        embeddings = self.embed(captions)\n",
    "        states = torch.stack([features]*(self.num_layers), dim=0)\n",
    "\n",
    "        #packed = pack_padded_sequence(embeddings, lengths, batch_first=False, enforce_sorted=True)    \n",
    "        \n",
    "        lstm_out, states = self.lstm(embeddings, states)\n",
    "        linear_outputs = self.linear(lstm_out)\n",
    "        \n",
    "        #outputs = linear_outputs.reshape(-1, self.vocab_size)\n",
    "        \n",
    "        return linear_outputs\n",
    "    \n",
    "    def generate_text(self, inputs, vocabulary, max_length=50):\n",
    "        result_text = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = inputs\n",
    "            lstm_in = features.unsqueeze(0)\n",
    "            hidden = None\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                lstm_out, hidden = self.lstm(lstm_in, hidden)\n",
    "                linear_in = lstm_out.squeeze(0)\n",
    "                linera_out = self.linear(linear_in)\n",
    "                \n",
    "                predicted = torch.argmax(linera_out, dim=1)\n",
    "                result_text.append(predicted.item())\n",
    "                \n",
    "                lstm_in = self.embed(predicted)\n",
    "                lstm_in = lstm_in.unsqueeze(0)\n",
    "\n",
    "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
    "                    break\n",
    "                \n",
    "        return [vocabulary.itos[idx] for idx in result_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNNtoRNN(nn.Module):\n",
    "class CNNtoLSTM(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, dropout):\n",
    "        super(CNNtoLSTM, self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size, dropout)\n",
    "        self.decoder = DecoderLSTM(embed_size, hidden_size, vocab_size, num_layers, dropout)\n",
    "    \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n",
    "    \n",
    "    def caption_image(self, image, vocabulary, max_length=50):\n",
    "        result_caption = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # image: (3, 224, 224)\n",
    "\n",
    "            features = self.encoder(image)                                    # inputs: (batch_size=1, embed_size)\n",
    "            lstm_in = features.unsqueeze(0)                                    # inputs: (1, batch_size=1, embed_size)\n",
    "            hidden = None\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                lstm_out, hidden = self.decoder.lstm(lstm_in, hidden)        # lstm_out: (1, batch_size=1, hidden_size)\n",
    "                linear_in = lstm_out.squeeze(0)                              # lstm_out: (batch_size=1, hidden_size)\n",
    "                linera_out = self.decoder.linear(linear_in)                      # output: (batch_size=1, vocab_size)\n",
    "                \n",
    "                predicted = torch.argmax(linera_out, dim=1)                     # predicted: (batch_size=1)\n",
    "                result_caption.append(predicted.item())\n",
    "                \n",
    "                lstm_in = self.decoder.embed(predicted)                      # input: (batch_size=1, embed_size)\n",
    "                lstm_in = lstm_in.unsqueeze(0)                                # input: (1, batch_size=1, embed_size)\n",
    "\n",
    "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
    "                    break\n",
    "                \n",
    "        return [vocabulary.itos[idx] for idx in result_caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "learning_rate = 3e-4\n",
    "batch_size = 64\n",
    "num_workers = 0\n",
    "dropout = 0.0\n",
    "\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_to_use = \"PCCD\"\n",
    "dataset_to_use = \"flickr8k\"\n",
    "#dataset_to_use = \"AVA\"\n",
    "\n",
    "if dataset_to_use == \"PCCD\":\n",
    "    imgs_folder = \"../datasets/PCCD/images/full\"\n",
    "    annotation_file = \"../datasets/PCCD/raw.json\"\n",
    "    test_file = \"../datasets/PCCD/images/PCCD_test.txt\"\n",
    "    \n",
    "elif dataset_to_use == \"flickr8k\":\n",
    "    imgs_folder = \"../datasets/flickr8k/images\"\n",
    "    annotation_file = \"../datasets/flickr8k/captions.txt\"\n",
    "    test_file = \"../datasets/flickr8k/flickr8k_test.txt\"\n",
    "\n",
    "elif dataset_to_use == \"AVA\":\n",
    "    imgs_folder = \"../datasets/AVA/images\"\n",
    "    #annotation_file = \"../datasets/AVA/CLEAN_AVA_FULL_COMMENTS.feather\"\n",
    "    annotation_file = \"../datasets/AVA/CLEAN_AVA_SAMPLE_COMMENTS.feather\"\n",
    "    test_file = \"../datasets/AVA/AVA_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((356,356)),\n",
    "        transforms.RandomCrop((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_loader, dataset = get_loader(\n",
    "    dataset_to_use=dataset_to_use,\n",
    "    imgs_folder=imgs_folder,\n",
    "    annotation_file=annotation_file,\n",
    "    test_file=test_file,\n",
    "    transform=transform,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    freq_threshold=8,\n",
    ")\n",
    "vocab_size = len(dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(vocab_size - 500, vocab_size):\n",
    "    #print(dataset.vocab.itos[i])\n",
    "    \n",
    "for i in range(500):\n",
    "    print(dataset.vocab.itos[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")           ## Nvidia CUDA Acceleration\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")    ## Apple M1 Metal Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model, loss, etc\n",
    "model = CNNtoLSTM(\n",
    "    embed_size=embed_size, \n",
    "    hidden_size=hidden_size, \n",
    "    vocab_size=vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    "    ).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "save_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_model:\n",
    "    writer = SummaryWriter(os.path.join(\"../CNN-LSTM/runs\", dataset_to_use))\n",
    "step = 0\n",
    "\n",
    "if load_model:\n",
    "    step = load_checkpoint(torch.load(\"../CNN-LSTM/runs/checkpoint.path.tar\"), model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/633 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m imgs \u001b[39m=\u001b[39m imgs\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m captions \u001b[39m=\u001b[39m captions\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(imgs, captions)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m targets \u001b[39m=\u001b[39m captions\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, outputs\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb Cell 22\u001b[0m in \u001b[0;36mCNNtoLSTM.forward\u001b[0;34m(self, images, captions)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, images, captions):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(images)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(features, captions)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb Cell 22\u001b[0m in \u001b[0;36mDecoderLSTM.forward\u001b[0;34m(self, features, captions)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb#X30sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([features]\u001b[39m*\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)     \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb#X30sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m#packed = pack_padded_sequence(embeddings, lengths, batch_first=False, enforce_sorted=True)    \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb#X30sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m lstm_out, states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(embeddings, states)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb#X30sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m linear_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(lstm_out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/keenansamway/Documents/GitHub/COSC576-Project/CNN-LSTM/playground.ipynb#X30sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m#outputs = linear_outputs.reshape(-1, self.vocab_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/rnn.py:759\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[39mif\u001b[39;00m is_batched:\n\u001b[1;32m    757\u001b[0m     \u001b[39mif\u001b[39;00m (hx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m \u001b[39mor\u001b[39;00m hx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m):\n\u001b[1;32m    758\u001b[0m         msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mFor batched 3-D input, hx and cx should \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 759\u001b[0m                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39malso be 3-D but got (\u001b[39m\u001b[39m{\u001b[39;00mhx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D, \u001b[39m\u001b[39m{\u001b[39;00mhx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D) tensors\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    760\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    761\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "        #for idx, (imgs, captions, lengths) in enumerate(train_loader):\n",
    "        for idx, (imgs, captions, lengths) in tqdm(enumerate(train_loader), total=len(train_loader), leave=True):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            imgs = imgs.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            outputs = model(imgs, captions)\n",
    "            \n",
    "            targets = captions.view(-1)\n",
    "            \n",
    "            outputs = outputs.view(-1, outputs.shape[2])\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            if save_model:\n",
    "                writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
    "            step += 1\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if step % 1000 == 0:\n",
    "                print(\"Epoch [{}/[{}], Step [{}], Loss: {:.4f}\".format(epoch+1, num_epochs, step, loss.item()))\n",
    "            \n",
    "        if save_model:\n",
    "            checkpoint = {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"step\": step,\n",
    "            }\n",
    "            loc = \"../CNN-LSTM/runs/checkpoint.pth.tar\"\n",
    "            save_checkpoint(checkpoint, filename=loc)\n",
    "        print(\"Epoch [{}/[{}], Loss: {:.4f}\".format(epoch+1, num_epochs, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "    \"step\": step,\n",
    "}\n",
    "loc = \"../CNN-LSTM/runs/checkpoint.pth.tar\"\n",
    "save_checkpoint(checkpoint, filename=loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint(torch.load(\"../CNN-LSTM/runs/checkpoint.pth.tar\"), model, optimizer)\n",
    "\n",
    "print_examples(model, device, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate text from random initialization\n",
    "inputs = torch.rand(1, embed_size).to(device)\n",
    "outputs = model.decoder.generate_text(inputs, dataset.vocab)\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06179fa2b8cfafd36062f042ab1b5d69dd100e6b8d44f3b802d9de85bc5b3b99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
